{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A vanilla RNN trained to count number of 1's in a binary input stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Python imports\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "np.random.seed(seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  1.  0.  1.  0.  1.  0.  0.]\n",
      " [ 1.  1.  0.  1.  1.  1.  0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  1.  1.  0.  1.  1.  0.  1.]\n",
      " [ 1.  1.  0.  1.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  1.  0.  1.  1.]\n",
      " [ 0.  0.  1.  0.  0.  1.  1.  1.  1.  1.]\n",
      " [ 1.  0.  0.  1.  0.  0.  1.  0.  1.  1.]\n",
      " [ 1.  1.  1.  0.  0.  1.  0.  1.  1.  1.]\n",
      " [ 0.  1.  0.  1.  0.  0.  1.  1.  0.  1.]\n",
      " [ 0.  1.  1.  0.  1.  1.  0.  1.  1.  1.]\n",
      " [ 0.  0.  1.  1.  0.  1.  1.  1.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  1.  1.  0.  0.]\n",
      " [ 1.  1.  1.  0.  1.  0.  1.  0.  1.  1.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  1.  1.  0.]\n",
      " [ 0.  1.  1.  0.  0.  1.  0.  1.  1.  1.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0.  0.  1.  1.]\n",
      " [ 0.  1.  1.  0.  1.  1.  1.  0.  0.  0.]\n",
      " [ 1.  0.  1.  0.  0.  1.  0.  1.  1.  1.]\n",
      " [ 0.  0.  1.  1.  1.  1.  0.  0.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "#Create dataset\n",
    "\n",
    "n_samples = 20\n",
    "sequence_len = 10 # number of timesteps\n",
    "\n",
    "X = np.zeros((n_samples, sequence_len))\n",
    "\n",
    "for i in range(n_samples):\n",
    "    X[i, :] = np.around(np.random.rand(sequence_len)).astype(int)\n",
    "    \n",
    "#create targets for each sequence\n",
    "t = np.sum(X, axis = 1)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the forward step fucntions\n",
    "\n",
    "def update_state(Xk, Sk, Wx, Wrec):\n",
    "    \"\"\"\n",
    "    compute state k from previous state (Sk-1) and current input\n",
    "    state (Xk) by use of input weights (Wx) and recursive\n",
    "    weights (Wrec)\n",
    "    \"\"\"\n",
    "    return Xk * Wx + Sk * Wrec\n",
    "\n",
    "def forward_states(X, Wx, Wrec):\n",
    "    \"\"\"\n",
    "    unfold the network and compute all state activations\n",
    "    given the input (X) and input weights (Wx) and recursive\n",
    "    weights (Wrec).\n",
    "    Return the state activations in a matrix, the last column\n",
    "    S[:, -1] cpntains the final activations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the state matrix with initial state as 0\n",
    "    S = np.zeros((X.shape[0], X.shape[1]+1))\n",
    "    \n",
    "    # Use the recurrence relation defined by update_state\n",
    "    for k in range(X.shape[1]):\n",
    "        S[:,k+1]  = update_state(X[:,k], S[:,k], Wx, Wrec)\n",
    "    return S\n",
    "\n",
    "def cost(y, t):\n",
    "    # return MSE loss\n",
    "    return ((t-y)**2).sum() / n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_gradient(y, t):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the MSE cost fucntions with \n",
    "    respect to the output y.\n",
    "    \"\"\"\n",
    "    return 2.0 * (y - t) / n_samples\n",
    "\n",
    "def backward_gradient(X, S, grad_out, Wrec):\n",
    "    \"\"\"\n",
    "    Backpropogate the gradient computed at the output (grad_out)\n",
    "    through the network. Acuumulate the parameter gradients for\n",
    "    Wx and Wrec for each layer by addition. Return the parameter\n",
    "    gradients as a tuple, and the gradients at ouput of each\n",
    "    layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the array that stores the gradients of the cost with respect to the states.\n",
    "    grad_overtime = np.zeros((X.shape[0], X.shape[1]+1))\n",
    "    grad_overtime[:, -1] = grad_out\n",
    "    \n",
    "    # Set the gradient accumulations to 0\n",
    "    wx_grad = 0\n",
    "    wrec_grad = 0\n",
    "    \n",
    "    for k in range(X.shape[1], 0, -1):\n",
    "        # Compute the parameter gradients and accumulate the results\n",
    "        wx_grad += np.sum(grad_overtime[:, k] * X[:, k-1])\n",
    "        wrec_grad += np.sum(grad_overtime[:, k] * S[:, k-1])\n",
    "    \n",
    "    return (wx_grad, wrec_grad), grad_overtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Numerical gradient of 274.595429 is not close to the backpropagation gradient of 14.352012!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-98d7f924dc23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Raise error if the numerical grade is not close to the backprop gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_backprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Numerical gradient of {:.6f} is not close to the backpropagation gradient of {:.6f}!'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_backprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No gradient errors found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Numerical gradient of 274.595429 is not close to the backpropagation gradient of 14.352012!"
     ]
    }
   ],
   "source": [
    "# Perform gradient checking\n",
    "# Set the weight parameters used during gradient checking\n",
    "params = [1.2, 1.2]  # [wx, wRec]\n",
    "# Set the small change to compute the numerical gradient\n",
    "eps = 1e-7\n",
    "# Compute the backprop gradients\n",
    "S = forward_states(X, params[0], params[1])\n",
    "grad_out = output_gradient(S[:,-1], t)\n",
    "backprop_grads, grad_over_time = backward_gradient(X, S, grad_out, params[1])\n",
    "# Compute the numerical gradient for each parameter in the layer\n",
    "for p_idx, _ in enumerate(params):\n",
    "    grad_backprop = backprop_grads[p_idx]\n",
    "    # + eps\n",
    "    params[p_idx] += eps\n",
    "    plus_cost = cost(forward_states(X, params[0], params[1])[:,-1], t)\n",
    "    # - eps\n",
    "    params[p_idx] -= 2 * eps\n",
    "    min_cost = cost(forward_states(X, params[0], params[1])[:,-1], t)\n",
    "    # reset param value\n",
    "    params[p_idx] += eps\n",
    "    # calculate numerical gradient\n",
    "    grad_num = (plus_cost - min_cost) / (2*eps)\n",
    "    # Raise error if the numerical grade is not close to the backprop gradient\n",
    "    if not np.isclose(grad_num, grad_backprop):\n",
    "        raise ValueError('Numerical gradient of {:.6f} is not close to the backpropagation gradient of {:.6f}!'.format(float(grad_num), float(grad_backprop)))\n",
    "print('No gradient errors found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define plotting functions\n",
    "\n",
    "# Define points to annotate (wx, wRec, color)\n",
    "points = [(2,1,'r'), (1,2,'b'), (1,-2,'g'), (1,0,'c'), (1,0.5,'m'), (1,-0.5,'y')]\n",
    "\n",
    "def get_cost_surface(w1_low, w1_high, w2_low, w2_high, nb_of_ws, cost_func):\n",
    "    \"\"\"Define a vector of weights for which we want to plot the cost.\"\"\"\n",
    "    w1 = np.linspace(w1_low, w1_high, num=nb_of_ws)  # Weight 1\n",
    "    w2 = np.linspace(w2_low, w2_high, num=nb_of_ws)  # Weight 2\n",
    "    ws1, ws2 = np.meshgrid(w1, w2)  # Generate grid\n",
    "    cost_ws = np.zeros((nb_of_ws, nb_of_ws))  # Initialize cost matrix\n",
    "    # Fill the cost matrix for each combination of weights\n",
    "    for i in range(nb_of_ws):\n",
    "        for j in range(nb_of_ws):\n",
    "            cost_ws[i,j] = cost_func(ws1[i,j], ws2[i,j])\n",
    "    return ws1, ws2, cost_ws\n",
    "\n",
    "def plot_surface(ax, ws1, ws2, cost_ws):\n",
    "    \"\"\"Plot the cost in function of the weights.\"\"\"\n",
    "    surf = ax.contourf(ws1, ws2, cost_ws, levels=np.logspace(-0.2, 8, 30), cmap=cm.pink, norm=LogNorm())\n",
    "    ax.set_xlabel('$w_{in}$', fontsize=15)\n",
    "    ax.set_ylabel('$w_{rec}$', fontsize=15)\n",
    "    return surf\n",
    "\n",
    "def plot_points(ax, points):\n",
    "    \"\"\"Plot the annotation points on the given axis.\"\"\"\n",
    "    for wx, wRec, c in points:\n",
    "        ax.plot(wx, wRec, c+'o', linewidth=2)\n",
    "\n",
    "def get_cost_surface_figure(cost_func, points):\n",
    "    \"\"\"Plot the cost surfaces together with the annotated points.\"\"\"\n",
    "    # Plot figures\n",
    "    fig = plt.figure(figsize=(10, 4))   \n",
    "    # Plot overview of cost function\n",
    "    ax_1 = fig.add_subplot(1,2,1)\n",
    "    ws1_1, ws2_1, cost_ws_1 = get_cost_surface(-3, 3, -3, 3, 100, cost_func)\n",
    "    surf_1 = plot_surface(ax_1, ws1_1, ws2_1, cost_ws_1 + 1)\n",
    "    plot_points(ax_1, points)\n",
    "    ax_1.set_xlim(-3, 3)\n",
    "    ax_1.set_ylim(-3, 3)\n",
    "    # Plot zoom of cost function\n",
    "    ax_2 = fig.add_subplot(1,2,2)\n",
    "    ws1_2, ws2_2, cost_ws_2 = get_cost_surface(0, 2, 0, 2, 100, cost_func)\n",
    "    surf_2 = plot_surface(ax_2, ws1_2, ws2_2, cost_ws_2 + 1)\n",
    "    plot_points(ax_2, points)\n",
    "    ax_2.set_xlim(0, 2)\n",
    "    ax_2.set_ylim(0, 2)\n",
    "    # Show the colorbar\n",
    "    fig.subplots_adjust(right=0.8)\n",
    "    cax = fig.add_axes([0.85, 0.12, 0.03, 0.78])\n",
    "    cbar = fig.colorbar(surf_1, ticks=np.logspace(0, 8, 9), cax=cax)\n",
    "    cbar.ax.set_ylabel('$\\\\xi$', fontsize=15, rotation=0, labelpad=20)\n",
    "    cbar.set_ticklabels(['{:.0e}'.format(i) for i in np.logspace(0, 8, 9)])\n",
    "    fig.suptitle('Cost surface', fontsize=15)\n",
    "    return fig\n",
    "\n",
    "def plot_gradient_over_time(points, get_grad_over_time):\n",
    "    \"\"\"Plot the gradients of the annotated point and how the evolve over time.\"\"\"\n",
    "    fig = plt.figure(figsize=(6.5, 4))  \n",
    "    ax = plt.subplot(111)\n",
    "    # Plot points\n",
    "    for wx, wRec, c in points:\n",
    "        grad_over_time = get_grad_over_time(wx, wRec)\n",
    "        x = np.arange(-grad_over_time.shape[1]+1, 1, 1)\n",
    "        plt.plot(x, np.sum(grad_over_time, axis=0), c+'-', label='({0}, {1})'.format(wx, wRec), linewidth=1, markersize=8)\n",
    "    plt.xlim(0, -grad_over_time.shape[1]+1)\n",
    "    # Set up plot axis\n",
    "    plt.xticks(x)\n",
    "    plt.yscale('symlog')\n",
    "    plt.yticks([10**8, 10**6, 10**4, 10**2, 0, -10**2, -10**4, -10**6, -10**8])\n",
    "    plt.xlabel('timestep k', fontsize=12)\n",
    "    plt.ylabel('$\\\\frac{\\\\partial \\\\xi}{\\\\partial S_{k}}$', fontsize=20, rotation=0)\n",
    "    plt.grid()\n",
    "    plt.title('Unstability of gradient in backward propagation.\\n(backpropagate from left to right)')\n",
    "    # Set legend\n",
    "    leg = plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), frameon=False, numpoints=1)\n",
    "    leg.set_title('$(w_x, w_{rec})$', prop={'size':15})\n",
    "    \n",
    "def get_grad_over_time(wx, wRec):\n",
    "    \"\"\"Helper func to only get the gradient over time from wx and wRec.\"\"\"\n",
    "    S = forward_states(X, wx, wRec)\n",
    "    grad_out = output_gradient(S[:,-1], t).sum()\n",
    "    _, grad_over_time = backward_gradient(X, S, grad_out, wRec)\n",
    "    return grad_over_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot cost surface and gradients\n",
    "\n",
    "# Get and plot the cost surface figure with markers\n",
    "fig = get_cost_surface_figure(lambda w1, w2: cost(forward_states(X, w1, w2)[:,-1] , t), points)\n",
    "\n",
    "# Get the plots of the gradients changing by backpropagating.\n",
    "plot_gradient_over_time(points, get_grad_over_time)\n",
    "# Show figures\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
